{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgmJT1/FlLD0jv+mF3ySF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayponaganti/Decesion-tree/blob/main/url_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub   # download video from youtube\n",
        "!pip install --upgrade moviepy\n",
        "!pip install moviepy ## extract audio from the video\n",
        "!pip install openai-whisper # extract text from audio file\n",
        "!pip install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43Mrg85fR51",
        "outputId": "053ff2b1-5e46-4cb3-8841-df88a8e8078f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231117)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qhn3s9uah4sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "import os\n",
        "import moviepy.editor as mp\n",
        "from tkinter import Tk\n",
        "from tkinter.filedialog import askopenfilename\n",
        "import subprocess\n",
        "\n",
        "def download_youtube_video(url, output_path):\n",
        "    # Download video from YouTube\n",
        "    yt = YouTube(url)\n",
        "    ys = yt.streams.get_lowest_resolution()\n",
        "    ys.download(output_path=output_path)\n",
        "    return os.path.join(output_path, ys.default_filename)\n",
        "\n",
        "def extract_audio(video_path, audio_output_path):\n",
        "    # Extract audio from the video\n",
        "    video = mp.VideoFileClip(video_path)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(audio_output_path)\n",
        "\n",
        "def extract_text_from_audio(audio_path):\n",
        "    # Use Whisper to extract text from audio\n",
        "    output = subprocess.check_output([\"whisper\", audio_path, \"--model\", \"tiny.en\"])\n",
        "    text = output.decode(\"utf-8\")\n",
        "    return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #  enter your YouTube URL\n",
        "    url = input(\"enter url\")\n",
        "\n",
        "    # Download video from YouTube\n",
        "    output_path = \"/content/trail2.mp4\"  # path of the your directory\n",
        "    video_path = download_youtube_video(url, output_path)\n",
        "\n",
        "    # Extract audio from video\n",
        "    audio_path = \"/content/trail2.mp4/7.mp3\" # name of the your audio file\n",
        "    extract_audio(video_path, audio_path)\n",
        "\n",
        "    # Extract text from audio\n",
        "    extracted_text = extract_text_from_audio(audio_path)\n",
        "    print(\"Extracted Text:\")\n",
        "    print(extracted_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3alnCWB_0M1",
        "outputId": "c802e7f7-e4c4-46c8-9873-6b195e4cba21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.youtube.com/watch?v=a5yDWeSoudEhttps://www.youtube.com/watch?v=a5yDWeSoudE\n",
            "MoviePy - Writing audio in /content/trail2.mp4/7.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Extracted Text:\n",
            "[00:00.000 --> 00:04.360]  Hello, everyone, and welcome to my another tutorial.\n",
            "[00:04.360 --> 00:08.120]  Text recognition with TensorFlow and CTC Network.\n",
            "[00:08.120 --> 00:13.400]  So if you came to this tutorial, you should know that extracting text of different sizes,\n",
            "[00:13.400 --> 00:19.240]  shapes, and orientations for our images is a fundamental problem in many contexts, especially\n",
            "[00:19.240 --> 00:25.280]  in augmented reality-stent systems, e-commerce, and context moderation on social media\n",
            "[00:25.280 --> 00:26.520]  platforms.\n",
            "[00:26.520 --> 00:31.240]  To solve this problem, we need to extract tracks from images very accurately.\n",
            "[00:31.240 --> 00:36.080]  So there are two most popular methods that we could use to extract tags from images.\n",
            "[00:36.080 --> 00:37.240]  First, what?\n",
            "[00:37.240 --> 00:43.400]  Is we can localize text and images using tech detectors or segmentators, and then extract\n",
            "[00:43.400 --> 00:48.000]  localize a text, and that's way more straightforward.\n",
            "[00:48.000 --> 00:54.920]  And there is another way we can train a model that achieves both the tactile and recognition\n",
            "[00:54.920 --> 00:59.040]  with a single model, and that's a hard way, it's a hard core.\n",
            "[00:59.040 --> 01:05.880]  So in this tutorial, I will focus only a word extraction part from the whole OCR pipeline.\n",
            "[01:05.880 --> 01:12.240]  Because if I would like to cover everything, it might take a house of covering it and making\n",
            "[01:12.240 --> 01:14.880]  and writing tutorials about it, etc.\n",
            "[01:14.880 --> 01:21.160]  So, but it's a valid but no, the pipeline of the most popular OCR's available today,\n",
            "[01:21.160 --> 01:26.760]  as a set, most pipelines contain a text detection step and text recognition step.\n",
            "[01:26.760 --> 01:28.720]  So first, what does text detection?\n",
            "[01:28.720 --> 01:34.800]  Well, it actually helps to identify the location of an image that contains text.\n",
            "[01:34.800 --> 01:41.160]  It takes an image as input and gives boxes with coordinates as output vertex is.\n",
            "[01:41.160 --> 01:44.400]  And the second step is text recognition.\n",
            "[01:45.400 --> 01:51.680]  The step extracts text from an image input using bounding boxes derived from a text detection\n",
            "[01:51.680 --> 01:52.680]  model.\n",
            "[01:52.680 --> 01:58.840]  It inputs an image with cropped image part using the bounding boxes from the detector and\n",
            "[01:58.840 --> 02:00.800]  output a raw text.\n",
            "[02:00.800 --> 02:07.120]  So the text detection is very similar to the object detection test, where the object, which\n",
            "[02:07.120 --> 02:10.440]  needs to be detected, is nothing more but the text.\n",
            "[02:10.440 --> 02:15.880]  In such research, it has taken place in the field of text text out of image accurately.\n",
            "[02:15.880 --> 02:19.240]  And many detectors, the text are the world level.\n",
            "[02:19.240 --> 02:25.680]  However, the problem with world level text is that they fail to notice word of arbitrary\n",
            "[02:25.680 --> 02:29.880]  shape that are rotated, curves, tries to, etc.\n",
            "[02:29.880 --> 02:36.360]  But it was proven that we could achieve even better results by using various segmentation techniques\n",
            "[02:36.360 --> 02:39.200]  instead of using detectors.\n",
            "[02:39.200 --> 02:44.120]  During each character and the spacing with each characters, it helps to detect various\n",
            "[02:44.120 --> 02:45.960]  shaped text.\n",
            "[02:45.960 --> 02:49.920]  So for the text detection, you can choose other popular techniques.\n",
            "[02:49.920 --> 02:55.800]  But as I already mentioned, it would be too complex and an extensive tutorial to cover\n",
            "[02:55.800 --> 02:56.800]  both.\n",
            "[02:56.800 --> 03:03.600]  So I will focus only on exploring the CTC networks for text recognition in this tutorial.\n",
            "[03:03.600 --> 03:11.000]  So I notes that when developing various things, I must re-implement things by word using\n",
            "[03:11.000 --> 03:12.240]  over and over.\n",
            "[03:12.240 --> 03:18.960]  So why not simplifying this by creating a library to hold all this stuff.\n",
            "[03:18.960 --> 03:24.400]  So with this tutorial, I'm starting a new machine learning training utility code called\n",
            "[03:24.400 --> 03:31.000]  MLQ library that I will open source on GitHub, where I'll save all my tutorials that I'll\n",
            "[03:31.000 --> 03:33.360]  cover now and in the future.\n",
            "[03:33.360 --> 03:38.360]  And I'll publish them in the tutorials folder under this library.\n",
            "[03:38.360 --> 03:45.160]  You can find the link to this GitHub repository in my well description of this video.\n",
            "[03:45.160 --> 03:49.480]  So I will now jump straight to code, I'll do this step by step.\n",
            "[03:49.480 --> 03:55.720]  So after the text logologies instead, regions containing text are cropped and sent through\n",
            "[03:55.720 --> 03:59.680]  synand layers to extract image features.\n",
            "[03:59.680 --> 04:06.120]  These features are later fed into many many LSTM architecture that are put softmax probabilities\n",
            "[04:06.120 --> 04:08.040]  via the dictionary.\n",
            "[04:08.040 --> 04:13.800]  These outputs are different time steps are provided to the CTC decoder to obtain the raw\n",
            "[04:13.800 --> 04:15.720]  text from the images.\n",
            "[04:15.720 --> 04:22.280]  I will cover each step in detail in this tutorial, but a little later.\n",
            "[04:22.280 --> 04:28.480]  First, let's look at the my tensile model to understand how we connect the CNN layer\n",
            "[04:28.480 --> 04:30.280]  with LSTM layer.\n",
            "[04:30.280 --> 04:35.440]  This tutorial is not a beginner level because I'm here covering not everything step by step,\n",
            "[04:35.440 --> 04:37.240]  but only the most important parts.\n",
            "[04:37.240 --> 04:44.040]  I'll probably post some of the codes here in my text version tutorial, deeper code\n",
            "[04:44.040 --> 04:50.880]  parts, but if you want to get deeper, I recommend going straight to my GitHub repository\n",
            "[04:50.880 --> 04:55.800]  to my tutorials and to the first tutorial because I'm recording it right now.\n",
            "[04:55.800 --> 05:02.600]  Of course, the link to that GitHub is below in this video description and to run this,\n",
            "[05:02.600 --> 05:11.800]  of course, you need to either install this MLT library from a clone repository or either\n",
            "[05:11.800 --> 05:17.280]  you can do the pip install MLT equal 0.1.3.\n",
            "[05:17.280 --> 05:20.960]  It's recommended, I recommend this version for this tutorial.\n",
            "[05:20.960 --> 05:30.840]  So let's go to the code right now and let's look at what I have here.\n",
            "[05:30.840 --> 05:36.520]  And you might see that here is my train model and this MLT you and simply I'll start\n",
            "[05:36.520 --> 05:37.520]  with a model.\n",
            "[05:37.520 --> 05:44.920]  It's pretty simple and I don't know if you need something very hard.\n",
            "[05:44.920 --> 05:47.800]  So especially here is the input dimension.\n",
            "[05:47.800 --> 05:53.280]  It's my image dimensions, for example, it might be 32 by 128.\n",
            "[05:53.280 --> 05:58.800]  128 is the length of the image and that's pretty understandable here.\n",
            "[05:58.800 --> 06:06.760]  And now I borrowed some idea from ResNet implementation to make a better, convolutional network.\n",
            "[06:06.760 --> 06:09.280]  So they can roll better a little.\n",
            "[06:09.280 --> 06:15.440]  So here's a residual blocks that come from my MLT library, I simply put the blocks here.\n",
            "[06:15.440 --> 06:21.840]  So I believe I'll use this kind of architecture in coming five to no tutorials.\n",
            "[06:21.840 --> 06:25.880]  And this model, this really works good for me usually.\n",
            "[06:25.880 --> 06:31.480]  And as you can see here is the last hidden layer that gives us some kind of output and\n",
            "[06:31.480 --> 06:37.200]  we are not sure right now what is the output, but if we would create the model, we can\n",
            "[06:37.200 --> 06:41.040]  type model summary and it would give us something.\n",
            "[06:41.040 --> 06:46.440]  But the problem here is that here is three shapes.\n",
            "[06:46.440 --> 06:50.480]  I mean, there is three access and of course batches.\n",
            "[06:50.480 --> 06:58.720]  So it's for access and LSTM accepts only one batch layer and two additional layers.\n",
            "[06:58.720 --> 07:01.000]  So that's kind of simple.\n",
            "[07:01.000 --> 07:08.440]  What we need to do here is I need to reshape this and this I simply multiply two of these\n",
            "[07:08.440 --> 07:14.720]  access and one of them I leave and this is features access here.\n",
            "[07:14.720 --> 07:20.560]  And simply I receive this kind of squeezed output that I feed later to LSTM.\n",
            "[07:20.560 --> 07:28.360]  And after this, I receive the BLSTM that I can simply use as a softmax activation and use\n",
            "[07:28.360 --> 07:31.000]  a dance to give me the output for it.\n",
            "[07:31.000 --> 07:33.520]  As you can see, that's pretty simple.\n",
            "[07:34.520 --> 07:42.360]  I don't know, of course, it must be familiar to know what I'm doing here, but well, if\n",
            "[07:42.360 --> 07:47.040]  you're new to here, go step by step, run it in a bigger mode and you can understand everything\n",
            "[07:47.040 --> 07:48.880]  however, however you want.\n",
            "[07:48.880 --> 07:54.520]  So this might give you a limited information if you're familiar with TensorFlow, but the idea\n",
            "[07:54.520 --> 07:59.680]  here is to create the model with the correct output for CTC loss function.\n",
            "[07:59.680 --> 08:06.280]  So to do so, we must transition from CNN to LSTM, as I explained right now.\n",
            "[08:06.280 --> 08:11.680]  So to make everything right, we must ensure that the last layer meets the requirements\n",
            "[08:11.680 --> 08:13.880]  for CTC loss function.\n",
            "[08:13.880 --> 08:18.520]  Here I have for example 63 in my worst data set.\n",
            "[08:18.520 --> 08:20.920]  Well, this is my output dimension.\n",
            "[08:20.920 --> 08:27.800]  In my worst it's, I now have 62 different checks, but we need to increment it by one\n",
            "[08:27.800 --> 08:33.480]  because we need additional separation, the candidate is used for parting.\n",
            "[08:33.480 --> 08:41.200]  And padding is because when we feed batch to our network, the width and height of data\n",
            "[08:41.200 --> 08:42.200]  should match.\n",
            "[08:42.200 --> 08:45.720]  So it would be processed as metrics.\n",
            "[08:45.720 --> 08:47.880]  So that's like a must.\n",
            "[08:47.880 --> 08:50.640]  So that's it.\n",
            "[08:50.640 --> 08:58.040]  And for example, I also have two, five, six, as you may see here, and this is the multiplication\n",
            "[08:58.040 --> 09:05.840]  of my layers because here as output would be 8 and here would be something like a 32 and\n",
            "[09:05.840 --> 09:09.360]  when we multiply it's 256.\n",
            "[09:09.360 --> 09:12.840]  And that's quite, quite nice.\n",
            "[09:12.840 --> 09:21.560]  And it should be larger than your maximum word length in your label data set.\n",
            "[09:21.560 --> 09:24.480]  Because well, this is how CTC works.\n",
            "[09:24.480 --> 09:32.040]  It must be at least twice as larger than your word maximum word length in a data set.\n",
            "[09:32.040 --> 09:39.560]  Next, I must cover the LSTM, well a little bit, I'm not covering it deeply.\n",
            "[09:39.560 --> 09:48.320]  So this image, you can see that there is an input of 32 by 128 size.\n",
            "[09:48.320 --> 09:53.720]  They send through convolution layers, then these layers are designed so that as a result,\n",
            "[09:53.720 --> 09:58.720]  we often, I feature maps over the shape 8 by 32 by 64.\n",
            "[09:58.720 --> 10:02.280]  And none here is a batch size that could take any value.\n",
            "[10:02.280 --> 10:07.440]  And this is, well, this is exactly what you might see on your model output.\n",
            "[10:07.440 --> 10:10.920]  And this is, this can be easily reshaped.\n",
            "[10:10.920 --> 10:18.880]  So when I multiply 8 by 32 and use the 64 as my last mention, I reshaped to this dimension.\n",
            "[10:18.880 --> 10:23.600]  And then we can use this in to the input of my LSTM.\n",
            "[10:23.600 --> 10:31.440]  And this way, my final shape is by batch size number of time steps and word embeddings\n",
            "[10:31.440 --> 10:32.440]  dimension.\n",
            "[10:32.440 --> 10:39.000]  And that's it, my model is ready to be used for CTC lost with TensorFlow.\n",
            "[10:39.000 --> 10:43.600]  Later, these feature maps are fed to the LSTM model.\n",
            "[10:43.600 --> 10:49.200]  As shown in following image, you might be thinking now that LSTM models are known to work\n",
            "[10:49.200 --> 10:53.920]  with sequential data and how feature maps are sequential.\n",
            "[10:53.920 --> 10:59.920]  But the idea here is that CNN layers will shape the sequences while changing the view\n",
            "[10:59.920 --> 11:01.680]  with its layers.\n",
            "[11:01.680 --> 11:09.200]  As a result, we get a softmax probability over vocabulary from LSTM model for every time step.\n",
            "[11:09.200 --> 11:12.160]  In example, we have 256.\n",
            "[11:12.160 --> 11:19.120]  Now let us move on the exciting part of the tutorial on calculating the loss value for this\n",
            "[11:19.120 --> 11:20.920]  architecture setup.\n",
            "[11:20.920 --> 11:25.080]  Of course, it's very hard, so it might be hard for me to explain.\n",
            "[11:25.080 --> 11:29.440]  I'll give you a short explanation about CTC loss and why do we use it.\n",
            "[11:29.440 --> 11:35.040]  So this extensive topic for which I could create another whole explanation to talk, but\n",
            "[11:35.040 --> 11:41.080]  because there is already plenty of particles about CTC loss, I'll mention only the entire\n",
            "[11:41.080 --> 11:42.640]  and the idea about that.\n",
            "[11:42.640 --> 11:48.760]  Of course, we could create a data set with images or text, strings and then specify the corresponding\n",
            "[11:48.760 --> 11:52.040]  symbol for each horizontal image position.\n",
            "[11:52.040 --> 11:57.280]  We could train the neural networks to output the chart to score for each horizontal position\n",
            "[11:57.280 --> 11:59.840]  using the categorical horizontal.\n",
            "[11:59.840 --> 12:00.840]  As the loss.\n",
            "[12:00.840 --> 12:04.360]  However, there's a lot of problems that might face.\n",
            "[12:04.360 --> 12:09.920]  It's very time consuming and tedious to rotate data set and the chart to level.\n",
            "[12:09.920 --> 12:16.960]  It's very hard to separate, for example, words where we have hello and there's 2 L letters.\n",
            "[12:16.960 --> 12:22.240]  And there's many English words that and also other languages that are really hard to\n",
            "[12:22.240 --> 12:23.480]  annotate and separate.\n",
            "[12:23.480 --> 12:26.240]  And also, how about the speech recognition?\n",
            "[12:26.240 --> 12:33.360]  Can you imagine that you would need to annotate all the letters in the war and specify\n",
            "[12:33.360 --> 12:41.320]  in a spectrogram where is, for example, good, where is GOO and D separated and etc.\n",
            "[12:41.320 --> 12:47.000]  That would be, I don't know, would the hardest work in the world to do so.\n",
            "[12:47.000 --> 12:50.600]  So the CTC solved these problems for us.\n",
            "[12:50.600 --> 12:55.880]  So while training the model with CTC loss function, we only need to know the extract\n",
            "[12:55.880 --> 13:03.480]  word that is in the image or the sound is for which, therefore, we ignore both the position\n",
            "[13:03.480 --> 13:07.440]  the width of the symbol in the image or outer spectrogram.\n",
            "[13:07.440 --> 13:12.240]  The organized text does not require for the processing for us.\n",
            "[13:12.240 --> 13:16.680]  It's simply need to be converted from numerical to string format.\n",
            "[13:16.680 --> 13:17.680]  That's it.\n",
            "[13:17.680 --> 13:19.360]  We have the answer.\n",
            "[13:19.360 --> 13:25.300]  So to distinguish between two consecutive tokens and the applicator cans, a separation\n",
            "[13:25.300 --> 13:28.440]  to can is used as already mentioned before.\n",
            "[13:28.440 --> 13:32.280]  The best way to understand how it works would be to imagine that we are working with\n",
            "[13:32.280 --> 13:35.960]  a sound data that says, good, for example.\n",
            "[13:35.960 --> 13:42.040]  I don't know, it's really hard to explain this in a word, but in a text watch tool, it's\n",
            "[13:42.040 --> 13:48.760]  way easy to understand because when I write home, for example, it's really easy to write\n",
            "[13:48.760 --> 13:54.160]  the program home with for all and for example, to either end.\n",
            "[13:54.160 --> 13:59.160]  And when we remove these kind of duplicator cats, we assume hope.\n",
            "[13:59.160 --> 14:05.880]  So only that one idea is that we need to separate letters with some kind of separation\n",
            "[14:05.880 --> 14:06.880]  to can.\n",
            "[14:06.880 --> 14:11.840]  This is kind of done with CTC laws and CTC decoding.\n",
            "[14:11.840 --> 14:17.920]  And this is only what is necessary to do.\n",
            "[14:17.920 --> 14:24.000]  And it calculates the best database on the most probable symbol or time step when analyzing\n",
            "[14:24.000 --> 14:27.200]  LSTM output.\n",
            "[14:27.200 --> 14:28.520]  And that's it.\n",
            "[14:28.520 --> 14:36.720]  So first, I believe you interested in the CTC code, I also implemented it in my tutorial.\n",
            "[14:36.720 --> 14:41.600]  So we'll check this up and let's get back to the coding pile.\n",
            "[14:41.600 --> 14:50.760]  So I'll only cover some blocks as I mentioned before and I'm using the Keras CTC batch\n",
            "[14:50.760 --> 14:52.080]  encoding.\n",
            "[14:52.080 --> 14:58.120]  And if I'll open my training sphere, I'll go down and there is a CTC loss.\n",
            "[14:58.120 --> 15:00.200]  I'll open the CTC loss.\n",
            "[15:00.200 --> 15:06.680]  And as you can see, this is my CTC loss that I need simply to use as a regular was\n",
            "[15:07.680 --> 15:10.960]  loss we use in tensile flow models.\n",
            "[15:10.960 --> 15:16.280]  And we simply instead of another loss, we type this loss.\n",
            "[15:16.280 --> 15:22.760]  And this is kind of specific functions implemented in a tensile flow that does everything\n",
            "[15:22.760 --> 15:23.760]  for us.\n",
            "[15:23.760 --> 15:28.040]  And this loss will be used either in training and validation steps.\n",
            "[15:28.040 --> 15:34.720]  So you don't need to worry about what's going on here in CTC loss.\n",
            "[15:34.720 --> 15:40.320]  Well, except if you want to write it by yourself from scratch, if so, I'll post the\n",
            "[15:40.320 --> 15:42.640]  link in my text message toy.\n",
            "[15:42.640 --> 15:48.200]  Well, you can find more detailed explanation about how it works.\n",
            "[15:48.200 --> 15:55.840]  So let's go back to my training code and cover it a little from the top.\n",
            "[15:55.840 --> 15:57.640]  And here is my package.\n",
            "[15:57.640 --> 16:04.160]  I melt you as a mentioned and there I am inserting a lot of packages as data provider\n",
            "[16:04.160 --> 16:09.080]  and pre-process for transformers, losses, callbacks, metrics, and etc.\n",
            "[16:09.080 --> 16:16.040]  And here is my model from train model, as you can see, I already showed you and there\n",
            "[16:16.040 --> 16:17.880]  is a model configuration.\n",
            "[16:17.880 --> 16:24.000]  Well, this is kind of special configurations and we'll save all the configurations that\n",
            "[16:24.000 --> 16:26.640]  we use for training all models.\n",
            "[16:26.640 --> 16:30.440]  And the main idea that we need to save the vocabulary.\n",
            "[16:30.440 --> 16:36.040]  So as well, not the model, but also the vocabulary we'll save here.\n",
            "[16:36.040 --> 16:42.320]  And of course, there is this kind of data set, this a huge data set.\n",
            "[16:42.320 --> 16:50.760]  And I think it takes 10 gigabytes and of course, I don't know if you want to train this\n",
            "[16:50.760 --> 16:57.680]  stuff, but yeah, I don't know that it, I'll post the link in my text message toys.\n",
            "[16:57.680 --> 16:59.440]  Well, you can do loaded.\n",
            "[16:59.680 --> 17:08.120]  The idea here that there's a lot of images of text and you can see, Sandra, there's\n",
            "[17:08.120 --> 17:11.360]  a lot of them.\n",
            "[17:11.360 --> 17:17.520]  And I use all these words to train my model and that's really works.\n",
            "[17:17.520 --> 17:26.440]  And this is only a pre-processing step to pre-process this kind of data set.\n",
            "[17:26.440 --> 17:29.440]  And here you can see I'm saving train data set.\n",
            "[17:29.440 --> 17:39.880]  And it is saved here as a path to the image and as a label of that image correctly.\n",
            "[17:39.880 --> 17:45.640]  So here is the vocabulary, what is collected from all this data set and the maximum training\n",
            "[17:45.640 --> 17:46.640]  length.\n",
            "[17:46.640 --> 17:52.760]  And this is the maximum label length actually in my data set that I need to use.\n",
            "[17:52.760 --> 17:59.320]  I'm constructing my model because the shorter labels I need to, I'll need to add to this\n",
            "[17:59.320 --> 18:00.320]  length.\n",
            "[18:00.320 --> 18:05.600]  And then I'm saving these to my configurations as well when starting to train it, it will\n",
            "[18:05.600 --> 18:08.240]  be saved along the mode.\n",
            "[18:08.240 --> 18:13.400]  Then I use a data provider here and you might ask, what the hell is that?\n",
            "[18:13.400 --> 18:19.440]  So that's kind of my custom object that I created to simplify everything with our training\n",
            "[18:19.440 --> 18:20.440]  stuff.\n",
            "[18:20.440 --> 18:27.520]  If I open data provider, you might see that it inheritance from sequence class.\n",
            "[18:27.520 --> 18:32.520]  And there is a lot of parameters that you should be familiar with if you're thinking about\n",
            "[18:32.520 --> 18:33.920]  using it.\n",
            "[18:33.920 --> 18:40.080]  But the idea is here to simplify the training process here.\n",
            "[18:40.080 --> 18:44.240]  Because let's go back to the training and you understand here.\n",
            "[18:44.240 --> 18:49.520]  So for example, I have, I'm here initializing the data provider and I'm giving this train\n",
            "[18:49.520 --> 18:50.680]  data set.\n",
            "[18:50.680 --> 18:51.680]  And that's it.\n",
            "[18:51.680 --> 18:53.920]  I don't need to worry about it.\n",
            "[18:53.920 --> 18:57.200]  I need to give the path to images and labels.\n",
            "[18:57.200 --> 18:58.200]  And that's it.\n",
            "[18:58.200 --> 19:01.520]  I said here the batch size for my training stuff.\n",
            "[19:01.520 --> 19:05.720]  And here data pre-processors, it will be my image reader.\n",
            "[19:05.720 --> 19:12.880]  And the idea here is that it's simply a CB2 function that will read the images for my\n",
            "[19:12.880 --> 19:13.880]  disk.\n",
            "[19:13.880 --> 19:17.680]  And this might be any function if you're working with the voice sound.\n",
            "[19:17.680 --> 19:22.760]  If you're working with object detection or object segmentation, etc., you simply create\n",
            "[19:22.760 --> 19:24.800]  another image reader.\n",
            "[19:24.800 --> 19:28.920]  You give this object here and you are fine with that.\n",
            "[19:28.920 --> 19:34.520]  So then I use a transformers that I used after the data process.\n",
            "[19:34.520 --> 19:36.360]  And this is image resizer.\n",
            "[19:36.360 --> 19:40.160]  And this will resize all the images to the same size.\n",
            "[19:40.160 --> 19:43.280]  So it would be used for my trainer.\n",
            "[19:43.280 --> 19:45.520]  And then it's like label index.\n",
            "[19:45.520 --> 19:47.200]  My labels are strings.\n",
            "[19:47.200 --> 19:52.040]  And we know that TensorFlow doesn't accept a set strings as input.\n",
            "[19:52.040 --> 19:54.840]  I need to convert them to that integers.\n",
            "[19:54.840 --> 20:00.720]  So I use the vocabulary to index my labels and etc.\n",
            "[20:00.720 --> 20:03.040]  And the last step is label padding.\n",
            "[20:03.040 --> 20:08.400]  I need to pad all my labels to be the same length when I'm trading.\n",
            "[20:08.400 --> 20:14.360]  And that's I do the same stuff for my training data set and validation data set.\n",
            "[20:14.360 --> 20:21.080]  And they are both the same works or the same way that I am training, then find the training\n",
            "[20:21.080 --> 20:22.080]  model.\n",
            "[20:22.080 --> 20:26.400]  And so then I'm simply using the compile.\n",
            "[20:26.400 --> 20:28.760]  So that's how we work.\n",
            "[20:28.760 --> 20:36.840]  Oh, by the way, if I go back to my data provider, there's a one main loop that it's called\n",
            "[20:36.840 --> 20:39.600]  yet item when we are using fit.\n",
            "[20:39.600 --> 20:47.600]  And as you can see, it has a for loop that iterates data set batches.\n",
            "[20:47.600 --> 20:50.120]  And this is our data set.\n",
            "[20:50.120 --> 20:56.960]  And it's first, it calls the preprocessors and then it simply works with that.\n",
            "[20:56.960 --> 20:58.160]  That might be customized.\n",
            "[20:58.160 --> 21:01.880]  And then I use the augmentors and then the transformers.\n",
            "[21:02.880 --> 21:11.880]  You can play around them however young, but usually this kind of stuff works for any\n",
            "[21:11.880 --> 21:15.160]  implementation of data you want to train.\n",
            "[21:15.160 --> 21:17.160]  That's kind of simplifies everything.\n",
            "[21:17.160 --> 21:20.680]  You need to change the object simply here.\n",
            "[21:20.680 --> 21:24.440]  And then I here here, I define my model.\n",
            "[21:24.440 --> 21:31.480]  I, I said I compile it with my CTC loss and this see very metric.\n",
            "[21:31.480 --> 21:37.000]  And this is kind of custom metric and we all know, well, if you're first time here,\n",
            "[21:37.000 --> 21:43.600]  you don't, but for example, when we are training a model to recognize words, sentences\n",
            "[21:43.600 --> 21:47.800]  or sound, it's crucial to validate them.\n",
            "[21:47.800 --> 21:52.360]  And it's not the best way to validate them with a loss function.\n",
            "[21:52.360 --> 21:56.920]  It's better to use a child to error rate or word error rate.\n",
            "[21:56.920 --> 22:06.400]  This tells us how, how accurately we protect characters according to the true labels or\n",
            "[22:06.400 --> 22:11.440]  how similar our sentence is to, to our true label.\n",
            "[22:11.440 --> 22:19.040]  And that's it, that's, and this is what we do in our callbacks, which I kind of define\n",
            "[22:19.040 --> 22:20.040]  here.\n",
            "[22:20.040 --> 22:22.840]  And the most important is early stopping.\n",
            "[22:22.840 --> 22:28.160]  And as you can see here, I am using validation, check the error rate.\n",
            "[22:28.160 --> 22:33.760]  And if I go to my see there, it's, sir, check the error rate.\n",
            "[22:33.760 --> 22:40.640]  And I, I received this rate here and I can, now my callbacks are tracking if my\n",
            "[22:40.640 --> 22:44.440]  checked error rate is improving while training.\n",
            "[22:44.440 --> 22:47.440]  And if it's not, it will stop training here.\n",
            "[22:47.440 --> 22:54.040]  And then, of course, we are using model checkpoints to save the best model according\n",
            "[22:54.040 --> 22:56.800]  to the validation chart, which are error rate.\n",
            "[22:56.800 --> 23:04.640]  We use a train logger that a log, what's happening in each heap at epoch there.\n",
            "[23:04.640 --> 23:06.560]  And then I use tensor board.\n",
            "[23:06.560 --> 23:12.760]  And of course, I use reduce learning or play tail that reduces learning rate when it's\n",
            "[23:12.760 --> 23:14.600]  not improving.\n",
            "[23:14.600 --> 23:15.920]  And of course, let's step.\n",
            "[23:15.920 --> 23:20.920]  It's kind of custom, but I convert my models into on the next format.\n",
            "[23:20.920 --> 23:27.720]  And then it doesn't matter if I have a tensor flow or not, I can port take this model,\n",
            "[23:27.720 --> 23:31.440]  shift it to another device, for example, Raspberry Pi.\n",
            "[23:31.440 --> 23:39.600]  And I can run inference on my Raspberry Pi without tensor flow implementation with current\n",
            "[23:39.600 --> 23:40.600]  inference.\n",
            "[23:40.600 --> 23:41.600]  And that's it.\n",
            "[23:41.600 --> 23:46.600]  And as you can hear, I construct a fit function.\n",
            "[23:46.600 --> 23:55.120]  And I wait for it to finish the training process that, right, is simple and nothing magical.\n",
            "[23:55.120 --> 24:02.320]  And on the last step, I had my data provider holds our data sets.\n",
            "[24:02.320 --> 24:09.760]  And sometimes we want to say them because, you know, when we are investigating or\n",
            "[24:09.760 --> 24:15.640]  improving our model, change a model architecture, we want to train on the same data set.\n",
            "[24:15.640 --> 24:19.760]  And as you can see, this data has trade and validation.\n",
            "[24:19.760 --> 24:27.320]  So later we can load this CSV to our data provider and we don't need to iterate everything\n",
            "[24:27.320 --> 24:34.640]  from zero and we simply can work for the new working with the same stuff.\n",
            "[24:34.640 --> 24:39.560]  So I don't want to train this model because I already trained it.\n",
            "[24:39.560 --> 24:47.520]  And because it has 10 gigabytes, which I separate into training and testing, it's a pretty\n",
            "[24:47.520 --> 24:49.360]  huge data set.\n",
            "[24:49.360 --> 24:54.560]  And of course, I don't want to train it right now because it took a lot of time for me.\n",
            "[24:54.560 --> 24:59.680]  But of course, don't worry, I'll give a link for you where you can download my model.\n",
            "[24:59.680 --> 25:01.640]  And you can test this around.\n",
            "[25:01.640 --> 25:06.200]  So but let's go to my training models here.\n",
            "[25:06.200 --> 25:10.840]  And as you can see here, this is the model I train here.\n",
            "[25:10.840 --> 25:20.000]  And I'm interested to check what is the pencil board of my logs.\n",
            "[25:20.000 --> 25:35.320]  So if I open the tensor board, log there and click that, it should load the tensor board\n",
            "[25:35.320 --> 25:36.960]  with the logs.\n",
            "[25:36.960 --> 25:40.640]  It collected from training process.\n",
            "[25:40.640 --> 25:46.000]  And of course, because it might be a huge file, it takes some time.\n",
            "[25:46.000 --> 25:51.160]  I open my brain browser and I open this tensor board.\n",
            "[25:51.160 --> 25:55.600]  And as you can see, let's look how this curves look like.\n",
            "[25:55.600 --> 26:02.840]  And I want to look at the scholars without this kind of and let's make it larger for\n",
            "[26:02.840 --> 26:03.840]  you.\n",
            "[26:03.840 --> 26:06.120]  It will be easy to see.\n",
            "[26:06.120 --> 26:12.560]  So for example, you can see that it was zero or the first epoch, it started training.\n",
            "[26:13.560 --> 26:18.160]  And yeah, you can definitely see it was tracking child array.\n",
            "[26:18.160 --> 26:23.080]  And each epoch, it was decreasing and the validation is blue.\n",
            "[26:23.080 --> 26:28.520]  So the validation is even better than this orange color.\n",
            "[26:28.520 --> 26:34.720]  And this is only because that this data set might have a lot of issues.\n",
            "[26:34.720 --> 26:37.160]  It's not correct.\n",
            "[26:38.160 --> 26:40.360]  This was my only for learning purposes.\n",
            "[26:40.360 --> 26:48.520]  I don't know if I could use this data model for something else, but when I was, that's\n",
            "[26:48.520 --> 26:53.880]  the idea how you can use my code to train your own word detector.\n",
            "[26:53.880 --> 26:57.440]  It's like an OCR where you can begin working on.\n",
            "[26:57.440 --> 27:03.160]  So right now you can see that it was successfully trained and this model was saved here\n",
            "[27:03.160 --> 27:04.240]  to own the next.\n",
            "[27:04.240 --> 27:09.040]  So right and by the way, there is a 22 hours of training process.\n",
            "[27:09.040 --> 27:11.400]  That's why I said it takes a lot of time.\n",
            "[27:11.400 --> 27:15.920]  So I don't think this model and we can check some inference results.\n",
            "[27:15.920 --> 27:18.760]  Yeah, I have the data set.\n",
            "[27:18.760 --> 27:25.600]  So I hear this model and I go to my tutorials and here's the inference model.\n",
            "[27:25.600 --> 27:29.240]  And I already have all the links ready here.\n",
            "[27:29.240 --> 27:34.080]  And right now what I would need, there's only 20 examples.\n",
            "[27:34.080 --> 27:36.200]  There's a lot of these examples.\n",
            "[27:36.200 --> 27:42.320]  And I'll show this for you, I'll comment this and it will be your size.\n",
            "[27:42.320 --> 27:44.360]  And let's see what I'll get here.\n",
            "[27:44.360 --> 27:45.360]  Okay.\n",
            "[27:45.360 --> 27:47.360]  I'll need to close this tensor board.\n",
            "[27:47.360 --> 27:49.800]  I don't need this anymore.\n",
            "[27:49.800 --> 27:51.480]  And this tab.\n",
            "[27:51.480 --> 27:57.000]  So yeah, let's run this inference model here.\n",
            "[27:57.000 --> 28:04.720]  And we should see that each prediction it made a screen in a moment in initialize a little.\n",
            "[28:04.720 --> 28:08.680]  So let's wait.\n",
            "[28:08.680 --> 28:23.640]  Yeah, it is as you can see, it's moniker and it was, it has one error, as you can see.\n",
            "[28:23.640 --> 28:31.640]  it is moniker and the true labor is moniker and the prediction is moniker, well, at least\n",
            "[28:31.640 --> 28:34.440]  yet, it's my mistake, but still a mistake.\n",
            "[28:34.440 --> 28:38.160]  Let's look at another one example, a colesia.\n",
            "[28:38.160 --> 28:39.160]  This tick.\n",
            "[28:39.160 --> 28:41.840]  Well, that's chakra array to zero.\n",
            "[28:41.840 --> 28:45.000]  This means that it was predicted accurately.\n",
            "[28:45.000 --> 28:46.000]  And that's nice.\n",
            "[28:46.000 --> 28:47.800]  Another firestorm.\n",
            "[28:47.800 --> 28:51.440]  And we can clearly see that it missed the eye here.\n",
            "[28:51.440 --> 28:54.360]  So that's a mistake.\n",
            "[28:54.360 --> 29:02.760]  And let's see another example, for example, this one, PCI and label is beside predictions\n",
            "[29:02.760 --> 29:03.760]  based on well.\n",
            "[29:03.760 --> 29:05.720]  That's great.\n",
            "[29:05.720 --> 29:07.520]  We put chases.\n",
            "[29:07.520 --> 29:14.480]  And there is one error, report choices and instead, well, it's quite similar to all,\n",
            "[29:14.480 --> 29:18.200]  but you might see that it works amazing.\n",
            "[29:18.200 --> 29:23.600]  And this is a bad label, because it said it's big a tale.\n",
            "[29:23.600 --> 29:27.640]  And well, my prediction can't understand what the fuck is that.\n",
            "[29:27.640 --> 29:30.160]  So it here is only h h h.\n",
            "[29:30.160 --> 29:34.720]  So that's why I said there is many mistakes in this day, the said, and I know it said more\n",
            "[29:34.720 --> 29:41.480]  of them, but this is not true about covering these mistakes, then ladies and et cetera.\n",
            "[29:41.480 --> 29:42.480]  Stiff list.\n",
            "[29:42.480 --> 29:43.480]  That's pretty cool.\n",
            "[29:43.480 --> 29:46.960]  And you might see that works pretty amazing, right?\n",
            "[29:47.320 --> 29:54.240]  And right now, I don't want to run through the whole data set, but the chart, the\n",
            "[29:54.240 --> 29:57.800]  error rate of whole data set, it's pretty nice.\n",
            "[29:57.800 --> 30:03.120]  It's only, I don't remember it now, but it might be around 2% of error.\n",
            "[30:03.120 --> 30:05.600]  So this means that it works really nice.\n",
            "[30:05.600 --> 30:12.320]  And you can use this model to, well, transfer learning for your other models, but you\n",
            "[30:12.320 --> 30:16.040]  can find that your input shape should be the same as this model.\n",
            "[30:16.040 --> 30:21.120]  So it might be not the best case for all your solutions.\n",
            "[30:21.120 --> 30:24.680]  But in conclusion, that's the end of this tutorial.\n",
            "[30:24.680 --> 30:32.520]  I'll put everything on my GitHub and et cetera, but I need to end this tutorial, I'll need\n",
            "[30:32.520 --> 30:38.120]  to go to the more interesting and exciting stuff too.\n",
            "[30:38.120 --> 30:44.600]  So extracting tags from images is a complex problem that is important in various context,\n",
            "[30:44.600 --> 30:48.480]  including augmented draw, the e-commerce, and content moderation.\n",
            "[30:48.480 --> 30:54.320]  There are two main approaches to extract tags from images using text detectors or segmentation\n",
            "[30:54.320 --> 31:00.240]  techniques to localize text and training a single model that performs tech detection\n",
            "[31:00.240 --> 31:01.800]  and recognition.\n",
            "[31:01.800 --> 31:09.880]  This tutorial will actually focus on the latter approach combining, say, an LSTM layers with\n",
            "[31:09.880 --> 31:14.480]  a CTC loss function to extract tags from images.\n",
            "[31:14.480 --> 31:22.200]  The tutorial also introduces a new open source library called MLTU, and that will be used\n",
            "[31:22.200 --> 31:28.120]  to store code for future drawers, and of course, you can use it for your own projects.\n",
            "[31:28.120 --> 31:34.800]  So but mainly I will mention that I tried to be as short as possible and explain only\n",
            "[31:34.800 --> 31:38.680]  that most essential parts of this tutorial.\n",
            "[31:38.760 --> 31:45.000]  Going deeper, it could be expanded into several different tutorials, but I still need to do more,\n",
            "[31:45.000 --> 31:47.560]  and I believe you are waiting for more.\n",
            "[31:47.560 --> 31:54.280]  At the end of this tutorial, we finally have a working system OCR model to recognize\n",
            "[31:54.280 --> 32:01.160]  tags from our images as you saw, that we train on our own data set, that you can also do\n",
            "[32:01.160 --> 32:02.760]  with your own data set.\n",
            "[32:03.760 --> 32:08.760]  So this is only the first part of my tutorial series, where we learned how to train a\n",
            "[32:08.760 --> 32:13.120]  custom OCR to recognize tags from images.\n",
            "[32:13.120 --> 32:19.040]  From now on, we can move to another challenge in tasks, and I hope this tutorial was really\n",
            "[32:19.040 --> 32:27.440]  useful, and of course, you can share this tutorial, might you help your repository with\n",
            "[32:27.440 --> 32:30.400]  others, you can use by yourself and et cetera.\n",
            "[32:30.400 --> 32:35.360]  And if you really liked it, you can leave the comments below this video, you can ask\n",
            "[32:35.360 --> 32:39.920]  me anything, you can like this video, please smash the like button for this, and of\n",
            "[32:39.920 --> 32:42.800]  course, share it with anyone.\n",
            "[32:42.800 --> 32:48.280]  And of course, I'll continue working with this library, I wish to expand this and give you\n",
            "[32:48.280 --> 32:54.880]  a lot of tutorials how you can, well, just pick and play with them, because there is a lot\n",
            "[32:54.880 --> 33:02.800]  of stuff we can do in one place, and our knowledge will expand exponentially.\n",
            "[33:02.800 --> 33:07.480]  So thank you all for watching, I hope this tutorial was useful, and in the next tutorial,\n",
            "[33:07.480 --> 33:13.360]  I'll show you how to train this model to recognize captures from images.\n",
            "[33:13.360 --> 33:17.600]  So see you there, and I wish you a nice day, bye!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}